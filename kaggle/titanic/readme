最后使用方法：
read_data_forest
或 predict_griant_boosting

超参数调优：
from keras.wrappers.scikit_learn import KerasClassifier
from create_model import create_model
rf=KerasClassifier(build_fn=create_model,verbose=0,batch_size=16,nb_epoch=210)

def model_param_optimize(model,params,x,y):
    from sklearn.model_selection import GridSearchCV
    param_test1 = {}
    for param in params:  # max_features=5,n_estimators=40,max_depth=6,min_samples_leaf=13,min_samples_split=38,subsample=0.75
        # n_estimators=140,max_depth=4,min_samples_split=50
        gsearch1 = GridSearchCV(estimator=model, param_grid=param,n_jobs=-1)
        gsearch1.fit(x, y)
        # gsearch1.cv_results_,
        print(gsearch1.best_estimator_, gsearch1.best_params_, gsearch1.best_score_)
        param_test1.update(gsearch1.best_params_)
        rf.sk_params.update(gsearch1.best_params_)

    print(param_test1)
    return model


def model_param_optimize(model,params,x,y):
    from sklearn.model_selection import GridSearchCV
    param_test1 = {}
    for param in params:  # max_features=5,n_estimators=40,max_depth=6,min_samples_leaf=13,min_samples_split=38,subsample=0.75
        # n_estimators=140,max_depth=4,min_samples_split=50
        gsearch1 = GridSearchCV(estimator=model, param_grid=param, scoring='roc_auc', iid=False, cv=5)
        gsearch1.fit(x, y)
        # gsearch1.cv_results_,
        print(gsearch1.best_estimator_, gsearch1.best_params_, gsearch1.best_score_)
        param_test1.update(gsearch1.best_params_)

        for p in param:
            if p == 'min_samples_split' and 'max_depth' in param:
                continue
            setattr(model, p, gsearch1.best_params_[p])